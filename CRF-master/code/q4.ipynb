{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ac1e04",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'elif' statement on line 32 (166539856.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_objectives.append(objective_func(params, X_train, y_train, C))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'elif' statement on line 32\n"
     ]
    }
   ],
   "source": [
    "def objective_func(params, X, y, C):\n",
    "    W, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "    log_likelihood_sum = 0\n",
    "    for i in range(len(X)):\n",
    "        log_likelihood_sum += compute_log_likelihood(X[i], y[i], W, T)\n",
    "    norm_W = np.linalg.norm(W, axis=1)**2\n",
    "    norm_T = np.linalg.norm(T)**2\n",
    "    regularization_term = 0.5 * (np.sum(norm_W) + np.sum(norm_T))\n",
    "    return -C * log_likelihood_sum / len(X) + regularization_term\n",
    "\n",
    "def gradient_func(params, X, y, C):\n",
    "    W, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "    grad_W = np.zeros_like(W)\n",
    "    grad_T = np.zeros_like(T)\n",
    "    for i in range(len(X)):\n",
    "        grad_W += compute_grad_W(X[i], y[i], W, T)\n",
    "        grad_T += compute_grad_T(X[i], y[i], W, T)\n",
    "    grad_W = -C * grad_W / len(X) + W\n",
    "    grad_T = -C * grad_T / len(X) + T\n",
    "    return np.concatenate([grad_W.flatten(), grad_T.flatten()])\n",
    "\n",
    "def stochastic_train(X_train, y_train, X_test, y_test, method='sgd', **kwargs):\n",
    "    params = np.zeros((26*128 + 26*26))\n",
    "    effective_passes = []\n",
    "    train_objectives = []\n",
    "    test_errors = []\n",
    "    for i in range(num_iterations):\n",
    "        batch_indices = np.random.choice(len(X_train), size=batch_size, replace=False)\n",
    "        X_batch, y_batch = X_train[batch_indices], y_train[batch_indices]\n",
    "        if method == 'sgd':\n",
    "            params -= learning_rate * gradient_func(params, X_batch, y_batch, C)\n",
    "        elif method == 'momentum':\n",
    "            # Similar to sgd, but includes momentum update\n",
    "        train_objectives.append(objective_func(params, X_train, y_train, C))\n",
    "        test_errors.append(compute_test_error(params, X_test, y_test))\n",
    "        effective_passes.append(i * batch_size / len(X_train))\n",
    "    return effective_passes, train_objectives, test_errors\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(effective_passes, train_objectives, test_errors, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(effective_passes, train_objectives, label='Training Objective')\n",
    "    plt.xlabel('Effective Passes')\n",
    "    plt.ylabel('Objective Value')\n",
    "    plt.title('Training Objective Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(effective_passes, test_errors, label='Test Error')\n",
    "    plt.xlabel('Effective Passes')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.title('Test Error Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60d6e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(X, y, W, T):\n",
    "    \"\"\"\n",
    "    Compute the log likelihood for a given sequence and its labels.\n",
    "    \"\"\"\n",
    "    log_likelihood = 0.0\n",
    "    for i in range(len(y) - 1):\n",
    "        feature_vector = X[i]\n",
    "        transition_score = T[y[i], y[i+1]]\n",
    "        emission_score = np.dot(W[y[i]], feature_vector)\n",
    "        log_likelihood += (emission_score + transition_score)\n",
    "    # Subtract log partition function (Z) for normalization\n",
    "    # Assuming a function compute_log_z to compute log(Z)\n",
    "    log_z = compute_log_z(X, W, T)\n",
    "    log_likelihood -= log_z\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def compute_grad_W(X, y, W, T):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the log likelihood with respect to W.\n",
    "    \"\"\"\n",
    "    grad_W = np.zeros_like(W)\n",
    "    for i in range(len(y)):\n",
    "        feature_vector = X[i]\n",
    "        grad_W[y[i]] += feature_vector\n",
    "        # Subtract expected feature counts\n",
    "        # Assuming a function compute_expected_features that computes expected feature counts\n",
    "        expected_features = compute_expected_features(X, W, T)\n",
    "        grad_W -= expected_features\n",
    "    return grad_W\n",
    "\n",
    "def compute_grad_T(X, y, W, T):\n",
    "    num_states = T.shape[0]\n",
    "    grad_T = np.zeros_like(T)\n",
    "    \n",
    "    for sequence, labels in zip(X, y):\n",
    "        seq_len = len(labels)\n",
    "        for i in range(seq_len - 1):\n",
    "            current_label = labels[i]\n",
    "            next_label = labels[i + 1]\n",
    "            grad_T[current_label, next_label] += 1\n",
    "        \n",
    "        # Here, add logic to compute the expected transitions based on the sequence\n",
    "        # This might involve calling `compute_expected_transitions` or equivalent logic\n",
    "        # Ensure this function accounts for the variable length of `sequence`\n",
    "        expected_transitions = compute_expected_transitions([sequence], W, T)\n",
    "        grad_T -= expected_transitions  # Adjust this line as necessary based on your implementation\n",
    "    \n",
    "    # Normalize or adjust grad_T as required for your model\n",
    "    return grad_T\n",
    "\n",
    "\n",
    "def compute_test_error(params, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute test error given the current parameters.\n",
    "    \"\"\"\n",
    "    W, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "    errors = 0\n",
    "    total = 0\n",
    "    for i in range(len(X_test)):\n",
    "        # Assuming a function predict_labels to predict labels for a sequence\n",
    "        predicted_labels = predict_labels(X_test[i], W, T)\n",
    "        errors += np.sum(predicted_labels != y_test[i])\n",
    "        total += len(y_test[i])\n",
    "    return errors / total\n",
    "\n",
    "def compute_expected_transitions(X, W, T):\n",
    "    \"\"\"\n",
    "    Compute the expected number of transitions between states for the given data X.\n",
    "    \n",
    "    Args:\n",
    "    - X: The input sequences, a list of 2D numpy arrays.\n",
    "    - W: The weight matrix for the feature functions.\n",
    "    - T: The transition matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - expected_transitions: A matrix of expected transition counts.\n",
    "    \"\"\"\n",
    "    num_states = T.shape[0]  # Number of states\n",
    "    expected_transitions = np.zeros_like(T)  # Initialize expected transitions matrix\n",
    "\n",
    "    for sequence in X:  # Loop through each sequence\n",
    "        # Initialize forward and backward matrices\n",
    "        alpha = forward_pass(sequence, W, T)\n",
    "        beta = backward_pass(sequence, W, T)\n",
    "\n",
    "        # Compute the expected transitions for the current sequence\n",
    "        for i in range(len(sequence) - 1):\n",
    "            for s1 in range(num_states):\n",
    "                for s2 in range(num_states):\n",
    "                    # Probability of transitioning from state s1 to s2 at position i\n",
    "                    transition_prob = np.exp(W[s2] @ sequence[i + 1] + T[s1, s2])\n",
    "                    # Compute the expected count for this transition\n",
    "                    expected_count = alpha[i, s1] * transition_prob * beta[i + 1, s2]\n",
    "                    expected_transitions[s1, s2] += expected_count\n",
    "    \n",
    "    # Normalize the expected transitions to get probabilities\n",
    "    expected_transitions /= np.sum(expected_transitions)\n",
    "\n",
    "    return expected_transitions\n",
    "\n",
    "\n",
    "def sgd(params, grad_func, X_train, y_train, C, learning_rate, epochs, batch_size):\n",
    "    params_history = [params.copy()]\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data at the start of each epoch\n",
    "        shuffled_indices = np.random.permutation(len(X_train))\n",
    "        for start_idx in range(0, len(X_train), batch_size):\n",
    "            # Indices for the current batch\n",
    "            batch_indices = shuffled_indices[start_idx:start_idx+batch_size]\n",
    "\n",
    "            # Since X_train and y_train are lists of arrays, we need to select the batches differently\n",
    "            X_batch = [X_train[i] for i in batch_indices]\n",
    "            y_batch = [y_train[i] for i in batch_indices]\n",
    "\n",
    "            # Here you need to adjust how you calculate the gradient\n",
    "            # This might involve iterating through X_batch and y_batch\n",
    "            # Or altering grad_func to accept lists of sequences\n",
    "            gradient = grad_func(params, X_batch, y_batch, C)\n",
    "\n",
    "            # Update parameters\n",
    "            params -= learning_rate * gradient\n",
    "        \n",
    "        params_history.append(params.copy())\n",
    "    \n",
    "    return params, params_history\n",
    "\n",
    "\n",
    "def sgd_with_momentum(params, grad_func, X_train, y_train, C, learning_rate, momentum_coeff, epochs, batch_size):\n",
    "    # Initialize velocity vector for momentum\n",
    "    velocity = np.zeros_like(params)\n",
    "    params_history = [params.copy()]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        shuffled_indices = np.random.permutation(len(X_train))\n",
    "        for start_idx in range(0, len(X_train), batch_size):\n",
    "            batch_indices = shuffled_indices[start_idx:start_idx+batch_size]\n",
    "            X_batch = [X_train[i] for i in batch_indices]\n",
    "            y_batch = [y_train[i] for i in batch_indices]\n",
    "            \n",
    "            gradient = grad_func(params, X_batch, y_batch, C)\n",
    "            \n",
    "            # Update the velocity and parameters\n",
    "            velocity = momentum_coeff * velocity - learning_rate * gradient\n",
    "            params += velocity\n",
    "        \n",
    "        params_history.append(params.copy())\n",
    "\n",
    "    return params, params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eac119e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train(file_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/train.txt\"):\n",
    "    \"\"\"\n",
    "    Reads the train data from the file.\n",
    "    Each row corresponds to an example and is split into the label and the feature vector.\n",
    "    \"\"\"\n",
    "    from string import ascii_lowercase\n",
    "    mapping = {letter: idx for idx, letter in enumerate(ascii_lowercase)}\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:  # End of sequence\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []  # Reset for the next sequence\n",
    "\n",
    "    print(\"Number of training sequences:\", len(dataX))\n",
    "    print(\"First 5 sequences' labels:\\n\", dataY[:5])\n",
    "    \n",
    "    return list(zip(dataX, dataY))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_test(file_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/test.txt\"):\n",
    "    \"\"\"\n",
    "    Reads the test data from the file.\n",
    "    Each row corresponds to an example and is split into the label and the feature vector.\n",
    "    The function assumes that each example ends when a row with the third column less than 0 is encountered.\n",
    "    \"\"\"\n",
    "    from string import ascii_lowercase\n",
    "    mapping = {letter: idx for idx, letter in enumerate(ascii_lowercase)}\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:  # Skip the last empty line if it exists\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:  # Check for the end of a sequence\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []  # Reset for the next sequence\n",
    "\n",
    "    print(\"Number of test sequences:\", len(dataX))\n",
    "    print(\"First 5 sequences' labels:\\n\", dataY[:5])\n",
    "\n",
    "    return list(zip(dataX, dataY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c37045e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 3438\n",
      "First 5 sequences' labels:\n",
      " [array([ 0, 10,  4]), array([14, 12, 12,  0, 13,  3,  8, 13,  6]), array([ 4, 17, 14]), array([13,  4, 23, 15,  4,  2, 19,  4,  3]), array([ 4,  2, 11,  0, 17,  8, 13,  6])]\n",
      "Number of test sequences: 3439\n",
      "First 5 sequences' labels:\n",
      " [array([24, 11, 14, 15,  7, 14, 13,  4]), array([13, 22, 14, 17, 10,  0,  1, 11,  4]), array([ 2,  2, 14, 20, 13, 19,  0,  1,  8, 11,  8, 19, 24]), array([17,  8,  6,  7, 19,  5, 20, 11, 11, 24]), array([ 4,  2, 14, 12, 15, 17,  4, 18, 18])]\n"
     ]
    }
   ],
   "source": [
    "# Assuming read_train and read_test are already defined and loaded\n",
    "\n",
    "# Call the functions\n",
    "train_data = read_train()\n",
    "test_data = read_test()\n",
    "\n",
    "# Unpack the feature matrices and labels vectors from the list of tuples\n",
    "X_train = [x for x, _ in train_data]\n",
    "y_train = [y for _, y in train_data]\n",
    "\n",
    "X_test = [x for x, _ in test_data]\n",
    "y_test = [y for _, y in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1278671",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 26  # Number of labels\n",
    "feature_size = 128  # Assuming each input feature vector has a dimension of 128\n",
    "W_init = np.zeros((num_labels, feature_size))\n",
    "T_init = np.zeros((num_labels, num_labels))\n",
    "params_init = np.concatenate([W_init.flatten(), T_init.flatten()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30f0439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_func(params, X_batch, y_batch, C):\n",
    "    W = params[:num_labels*feature_size].reshape((num_labels, feature_size))\n",
    "    T = params[num_labels*feature_size:].reshape((num_labels, num_labels))\n",
    "    grad_W = compute_grad_W(X_batch, y_batch, W, T)\n",
    "    grad_T = compute_grad_T(X_batch, y_batch, W, T)\n",
    "    return np.concatenate([grad_W.flatten(), grad_T.flatten()])\n",
    "\n",
    "def compute_expected_features(X, W, T):\n",
    "    \"\"\"\n",
    "    Computes expected feature counts for a given dataset X, weights W, and transition matrix T.\n",
    "    This function is a placeholder and needs to be implemented according to the specific model.\n",
    "    \"\"\"\n",
    "    # Placeholder for the expected features calculation\n",
    "    expected_features = np.zeros_like(W)\n",
    "    \n",
    "    # Assuming here that each X[i] is a sequence of feature vectors\n",
    "    for sequence in X:\n",
    "        # Here you would compute the expected features for the sequence\n",
    "        # This might involve forward-backward calculations for sequence models like HMMs or CRFs\n",
    "        # For simplicity, let's assume we have a function that does this:\n",
    "        sequence_expected_features = forward_backward(sequence, W, T)\n",
    "        \n",
    "        # Aggregate the expected features from each sequence\n",
    "        expected_features += sequence_expected_features\n",
    "        \n",
    "    return expected_features\n",
    "\n",
    "def forward_backward(sequence, W, T):\n",
    "    # Initialize forward and backward matrices\n",
    "    forward = np.zeros((len(sequence), len(W)))\n",
    "    backward = np.zeros((len(sequence), len(W)))\n",
    "\n",
    "    # Initialize expected feature counts\n",
    "    expected_features = np.zeros_like(W)\n",
    "    \n",
    "    # Forward pass to fill the forward matrix\n",
    "    forward[0] = initialize_forward(sequence, W)  # This needs to be defined based on your model\n",
    "    for i in range(1, len(sequence)):\n",
    "        for current_state in range(len(W)):\n",
    "            for previous_state in range(len(W)):\n",
    "                # Update forward[i][current_state] with the transition probability from previous_state to current_state\n",
    "                # and the emission probability of sequence[i] given current_state\n",
    "                pass  # The actual logic involves dynamic programming updates\n",
    "\n",
    "    # Backward pass to fill the backward matrix\n",
    "    backward[-1] = 1  # Typically initialized to 1 for all states\n",
    "    for i in range(len(sequence) - 2, -1, -1):\n",
    "        for current_state in range(len(W)):\n",
    "            for next_state in range(len(W)):\n",
    "                # Update backward[i][current_state] similar to the forward pass but in reverse\n",
    "                pass  # The actual logic involves dynamic programming updates\n",
    "\n",
    "    # Compute the expected features using forward and backward matrices\n",
    "    for i in range(len(sequence)):\n",
    "        for state in range(len(W)):\n",
    "            # Update expected_features based on the forward and backward probabilities\n",
    "            # and the feature vector of sequence[i]\n",
    "            pass  # Incorporate logic to accumulate expected feature counts\n",
    "\n",
    "    return expected_features\n",
    "\n",
    "def initialize_forward(sequence, W):\n",
    "    \"\"\"\n",
    "    Initialize the forward matrix for the first position in the sequence.\n",
    "    \n",
    "    Args:\n",
    "    - sequence: The input sequence for which forward probabilities are computed.\n",
    "    - W: The weight matrix of shape (num_states, num_features) for the CRF model.\n",
    "    \n",
    "    Returns:\n",
    "    - forward_initial: The initialized forward probabilities for the first position.\n",
    "    \"\"\"\n",
    "    num_states = W.shape[0]  # Assuming W's shape is (num_states, num_features)\n",
    "    \n",
    "    # Option 1: Uniform probabilities for the first position\n",
    "    forward_initial = np.ones(num_states) / num_states\n",
    "    \n",
    "    return forward_initial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb3cd436",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forward_pass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Run SGD\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m params_sgd \u001b[38;5;241m=\u001b[39m sgd(params_init, gradient_func, X_train, y_train, C, learning_rate, epochs, batch_size)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Run SGD with Momentum\u001b[39;00m\n\u001b[0;32m     11\u001b[0m params_sgd_momentum \u001b[38;5;241m=\u001b[39m sgd_with_momentum(params_init, gradient_func, X_train, y_train, C, learning_rate, momentum_coeff, epochs, batch_size)\n",
      "Cell \u001b[1;32mIn[42], line 119\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, grad_func, X_train, y_train, C, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[0;32m    114\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m [y_train[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Here you need to adjust how you calculate the gradient\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# This might involve iterating through X_batch and y_batch\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Or altering grad_func to accept lists of sequences\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m gradient \u001b[38;5;241m=\u001b[39m grad_func(params, X_batch, y_batch, C)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m    122\u001b[0m params \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradient\n",
      "Cell \u001b[1;32mIn[46], line 5\u001b[0m, in \u001b[0;36mgradient_func\u001b[1;34m(params, X_batch, y_batch, C)\u001b[0m\n\u001b[0;32m      3\u001b[0m T \u001b[38;5;241m=\u001b[39m params[num_labels\u001b[38;5;241m*\u001b[39mfeature_size:]\u001b[38;5;241m.\u001b[39mreshape((num_labels, num_labels))\n\u001b[0;32m      4\u001b[0m grad_W \u001b[38;5;241m=\u001b[39m compute_grad_W(X_batch, y_batch, W, T)\n\u001b[1;32m----> 5\u001b[0m grad_T \u001b[38;5;241m=\u001b[39m compute_grad_T(X_batch, y_batch, W, T)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate([grad_W\u001b[38;5;241m.\u001b[39mflatten(), grad_T\u001b[38;5;241m.\u001b[39mflatten()])\n",
      "Cell \u001b[1;32mIn[42], line 46\u001b[0m, in \u001b[0;36mcompute_grad_T\u001b[1;34m(X, y, W, T)\u001b[0m\n\u001b[0;32m     41\u001b[0m         grad_T[current_label, next_label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Here, add logic to compute the expected transitions based on the sequence\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# This might involve calling `compute_expected_transitions` or equivalent logic\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Ensure this function accounts for the variable length of `sequence`\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     expected_transitions \u001b[38;5;241m=\u001b[39m compute_expected_transitions([sequence], W, T)\n\u001b[0;32m     47\u001b[0m     grad_T \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m expected_transitions  \u001b[38;5;66;03m# Adjust this line as necessary based on your implementation\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Normalize or adjust grad_T as required for your model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 84\u001b[0m, in \u001b[0;36mcompute_expected_transitions\u001b[1;34m(X, W, T)\u001b[0m\n\u001b[0;32m     80\u001b[0m expected_transitions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(T)  \u001b[38;5;66;03m# Initialize expected transitions matrix\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m X:  \u001b[38;5;66;03m# Loop through each sequence\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# Initialize forward and backward matrices\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m forward_pass(sequence, W, T)\n\u001b[0;32m     85\u001b[0m     beta \u001b[38;5;241m=\u001b[39m backward_pass(sequence, W, T)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Compute the expected transitions for the current sequence\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'forward_pass' is not defined"
     ]
    }
   ],
   "source": [
    "C = 1000  # The best value found in the previous section\n",
    "learning_rate = 0.01  # Hyperparameter, tune accordingly\n",
    "momentum_coeff = 0.9  # Momentum coefficient, tune accordingly\n",
    "batch_size = 32  # Batch size for stochastic mini-batch\n",
    "epochs = 100  # Number of epochs\n",
    "\n",
    "# Run SGD\n",
    "params_sgd = sgd(params_init, gradient_func, X_train, y_train, C, learning_rate, epochs, batch_size)\n",
    "\n",
    "# Run SGD with Momentum\n",
    "params_sgd_momentum = sgd_with_momentum(params_init, gradient_func, X_train, y_train, C, learning_rate, momentum_coeff, epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_sgd = compute_test_error(params_sgd, X_test, y_test)\n",
    "test_error_sgd_momentum = compute_test_error(params_sgd_momentum, X_test, y_test)\n",
    "print(f\"Test Error (SGD): {test_error_sgd}\")\n",
    "print(f\"Test Error (SGD with Momentum): {test_error_sgd_momentum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eca8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the decline of training objective values\n",
    "def plot_training_objective(train_objectives, labels, title='Training Objective Decline'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for objectives, label in zip(train_objectives, labels):\n",
    "        plt.plot(objectives, label=label)\n",
    "    plt.xlabel('Effective Number of Passes')\n",
    "    plt.ylabel('Training Objective Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot test errors\n",
    "def plot_test_errors(test_errors, labels, title='Test Error'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for errors, label in zip(test_errors, labels):\n",
    "        plt.plot(errors, label=label)\n",
    "    plt.xlabel('Effective Number of Passes')\n",
    "    plt.ylabel('Word-wise Test Error (%)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Combine data for plotting\n",
    "train_objectives = [train_objective_values_sgd, train_objective_values_sgd_momentum, train_objective_values_lbfgs]\n",
    "test_errors = [test_errors_sgd, test_errors_sgd_momentum, test_errors_lbfgs]\n",
    "labels = ['SGD', 'SGD with Momentum', 'L-BFGS']\n",
    "\n",
    "# Plot\n",
    "plot_training_objective(train_objectives, labels)\n",
    "plot_test_errors(test_errors, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b26f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "803a959e",
   "metadata": {},
   "source": [
    "# from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e2ec20",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_train_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataX, dataY)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Now let's load the train and test data\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m read_train(train_data_path)\n\u001b[0;32m     48\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m read_test(test_data_path)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Implement the callback function to track the progress during optimization\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mread_train\u001b[1;34m(train_data_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_train\u001b[39m(train_data_path):\n\u001b[0;32m     10\u001b[0m     mapping \u001b[38;5;241m=\u001b[39m {letter: index \u001b[38;5;28;01mfor\u001b[39;00m index, letter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabcdefghijklmnopqrstuvwxyz\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(train_data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         raw_data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     dataX, dataY \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_train_data.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the paths for the train and test data files\n",
    "train_data_path = \"path_to_your_train_data.txt\"  # Update this path\n",
    "test_data_path = \"path_to_your_test_data.txt\"  # Update this path\n",
    "\n",
    "# Adaptation of read_train function to load training data\n",
    "def read_train(train_data_path):\n",
    "    mapping = {letter: index for index, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "    with open(train_data_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []\n",
    "    \n",
    "    return zip(dataX, dataY)\n",
    "\n",
    "# Adaptation of read_train function to load test data\n",
    "def read_test(test_data_path):\n",
    "    mapping = {letter: index for index, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "    with open(test_data_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []\n",
    "    \n",
    "    return zip(dataX, dataY)\n",
    "\n",
    "# Now let's load the train and test data\n",
    "train_dataset = read_train(train_data_path)\n",
    "test_dataset = read_test(test_data_path)\n",
    "\n",
    "# Implement the callback function to track the progress during optimization\n",
    "function_evaluations = 0\n",
    "def callback(params, *args):\n",
    "    global function_evaluations\n",
    "    function_evaluations += 1\n",
    "    \n",
    "    # Unpack arguments (assuming test dataset and possibly other args are passed)\n",
    "    X_test, y_test = args[:2]\n",
    "    \n",
    "    # Implement or call your test error function here\n",
    "    test_error = compute_test_error(params, X_test, y_test)\n",
    "    \n",
    "    # Implement or call your training objective function here\n",
    "    training_objective = compute_training_objective(params, X_train, y_train)\n",
    "    \n",
    "    # Print out the current progress\n",
    "    print(f\"Iteration {function_evaluations}: Test Error = {test_error}, Training Objective = {training_objective}\")\n",
    "    \n",
    "    # Optional: Store metrics for later analysis and plotting\n",
    "    test_errors.append(test_error)\n",
    "    train_objectives.append(training_objective)\n",
    "\n",
    "# Initialize lists to store the progress\n",
    "test_errors = []\n",
    "train_objectives = []\n",
    "\n",
    "# Note: You would also need to implement or provide the `compute_test_error` and `compute_training_objective` functions.\n",
    "# Replace \"X_train, y_train\" with the actual variables containing your training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce26ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model(file_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/model.txt\"):\n",
    "    \"\"\"\n",
    "    Reads the model data from the file.\n",
    "    The data consists of weight vectors for each label and a transition matrix T.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    W = np.array(raw_data[:26 * 128], dtype=float).reshape(26, 128)\n",
    "    T = np.array(raw_data[26 * 128:-1], dtype=float).reshape(26, 26)\n",
    "    T = np.swapaxes(T, 0, 1)\n",
    "    \n",
    "    print(\"Shapes of model data:\")\n",
    "    print(\"W:\", W.shape, \"T:\", T.shape)\n",
    "    #print(\"Top 5 rows of W:\\n\", W[:5])\n",
    "    \n",
    "    return W, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a889a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96780a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 3438\n",
      "First 5 sequences' labels:\n",
      " [array([ 0, 10,  4]), array([14, 12, 12,  0, 13,  3,  8, 13,  6]), array([ 4, 17, 14]), array([13,  4, 23, 15,  4,  2, 19,  4,  3]), array([ 4,  2, 11,  0, 17,  8, 13,  6])]\n",
      "Shapes of model data:\n",
      "W: (26, 128) T: (26, 26)\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 6331.055729\n",
      "         Iterations: 2\n",
      "         Function evaluations: 3\n",
      "         Gradient evaluations: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy, data_read, prob_grad\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "X_y = read_train()\n",
    "W, T = read_model()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def func(params, *args):\n",
    "    \"\"\"\n",
    "    Objective function specified in the handout.\n",
    "\n",
    "    Parameters:\n",
    "    - params: Array containing model parameters (W and T)\n",
    "    - args: Additional arguments (data and regularization parameter C)\n",
    "\n",
    "    Returns:\n",
    "    - Value of the objective function\n",
    "    \"\"\"\n",
    "    # Unpack model parameters\n",
    "    W, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "    data = args[0]  # Training data\n",
    "    C = args[1]     # Regularization parameter\n",
    "\n",
    "    # Compute the log sum\n",
    "    log_sum = 0\n",
    "    for example in data:\n",
    "        log_sum += prob_grad.compute_log_p(example[0], example[1], W, T)\n",
    "\n",
    "    # Compute the L2 norm of each row of W\n",
    "    norm = np.linalg.norm(W, axis=1) ** 2\n",
    "\n",
    "    # Compute the objective function value\n",
    "    objective_value = -1 * (C / len(data)) * log_sum + 0.5 * np.sum(norm) + 0.5 * np.sum(np.square(T))\n",
    "\n",
    "    return objective_value\n",
    "\n",
    "def func_prime(params, *args):\n",
    "    \"\"\"\n",
    "    Derivative of the objective function specified in the handout.\n",
    "\n",
    "    Parameters:\n",
    "    - params: Array containing model parameters (W and T)\n",
    "    - args: Additional arguments (data and regularization parameter C)\n",
    "\n",
    "    Returns:\n",
    "    - Gradient of the objective function\n",
    "    \"\"\"\n",
    "    # Unpack model parameters\n",
    "    W, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "    data = args[0]  # Training data\n",
    "    C = args[1]     # Regularization parameter\n",
    "\n",
    "    # Initialize gradients\n",
    "    log_grad_w = np.zeros((26, 128))\n",
    "    log_grad_t = np.zeros((26, 26))\n",
    "\n",
    "    # Compute the gradient of logP w.r.t. W and T\n",
    "    for example in data:\n",
    "        log_grad_w += prob_grad.log_p_wgrad(W, example[0], example[1], T)\n",
    "        log_grad_t += prob_grad.log_p_tgrad(T, example[0], example[1], W)\n",
    "\n",
    "    # Multiply by -C/N\n",
    "    log_grad_w *= -1 * C / len(data)\n",
    "    log_grad_t *= -1 * C / len(data)\n",
    "\n",
    "    # Add gradient of norm\n",
    "    log_grad_w += W\n",
    "\n",
    "    # Add normalizing factor\n",
    "    log_grad_t += T\n",
    "\n",
    "    # Flatten and concatenate gradients\n",
    "    gradient = np.concatenate([log_grad_w.reshape(26*128), log_grad_t.reshape(26*26)])\n",
    "\n",
    "    return gradient\n",
    "\n",
    "\n",
    "on = numpy.concatenate([W.reshape(26*128), T.reshape(26*26)])\n",
    "\n",
    "res = func(on, X_y[:9], 1000)\n",
    "result = func_prime(on, X_y[:9], 1000)\n",
    "\n",
    "#need to flatten for the optimizer\n",
    "initial_guess = numpy.zeros((26*128+26*26))\n",
    "\n",
    "bounds = [(-10000000, 10000000)]*(28*128+26*26)\n",
    "\n",
    "\n",
    "ret = fmin_bfgs(func, initial_guess, fprime=func_prime, args=(X_y[:5], 1000), maxiter=2, retall=True, full_output=True)\n",
    "\n",
    "def get_params(x_y):\n",
    "    t0 = time.time()\n",
    "    ret=fmin_bfgs(func, initial_guess, fprime=func_prime, args=(x_y,1000), maxiter=1,retall=True, full_output=True)\n",
    "    t1 = time.time()\n",
    "    with open(\"best_Weights_tampered\",\"+bw\") as f :\n",
    "        pickle.dump(ret,f)\n",
    "    numpy.savetxt(\"best_Weights_tampered\",ret[0])\n",
    "    #numpy.savetxt(\"best_func_c_10\",ret[1])\n",
    "    \n",
    "    print(f\"Time: {t1-t0}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2e7a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import prob_grad\n",
    "import data_read\n",
    "\n",
    "def sgd_step(params, X_batch, y_batch, C, learning_rate):\n",
    "    # We need to pack X_batch and y_batch into data because func_prime expects it that way\n",
    "    data = (X_batch, y_batch)\n",
    "    gradient = func_prime(params, data, C)\n",
    "    params -= learning_rate * gradient\n",
    "    return params\n",
    "\n",
    "def sgd_momentum_step(params, velocity, X_batch, y_batch, C, learning_rate, momentum_coeff):\n",
    "    # Similar packing of X_batch and y_batch into data\n",
    "    data = (X_batch, y_batch)\n",
    "    gradient = func_prime(params, data, C)\n",
    "    velocity = momentum_coeff * velocity - learning_rate * gradient\n",
    "    params += velocity\n",
    "    return params, velocity\n",
    "\n",
    "\n",
    "# Implementation of the error computation function for a CRF\n",
    "def compute_error(W, T, x_seq, y_seq):\n",
    "    # Assuming that W and T are your model's parameters\n",
    "    # x_seq is a single sequence of observations\n",
    "    # y_seq is the corresponding sequence of true labels\n",
    "    # We will calculate error as the number of incorrect label predictions\n",
    "    error_count = 0\n",
    "    # Assume you have a function that can give you the predicted sequence based on W, T, and x_seq\n",
    "    predicted_seq = prob_grad.predict_sequence(W, T, x_seq)\n",
    "    for true_label, predicted_label in zip(y_seq, predicted_seq):\n",
    "        if true_label != predicted_label:\n",
    "            error_count += 1\n",
    "    return error_count\n",
    "\n",
    "def compute_test_error(W, T, X_test, y_test):\n",
    "    total_errors = 0\n",
    "    total_labels = 0\n",
    "    for x_seq, y_seq in zip(X_test, y_test):\n",
    "        total_errors += compute_error(W, T, x_seq, y_seq)\n",
    "        total_labels += len(y_seq)\n",
    "    return total_errors / total_labels  # return the error rate\n",
    "\n",
    "# You can use similar logic to compute the training objective in a loop for each sequence.\n",
    "# For example:\n",
    "def compute_training_objective(W, T, X_train, y_train, C):\n",
    "    # Assuming that compute_log_p gives log probability for a sequence\n",
    "    total_log_p = 0\n",
    "    for x_seq, y_seq in zip(X_train, y_train):\n",
    "        total_log_p += prob_grad.compute_log_p(W, T, x_seq, y_seq)\n",
    "    # Add the regularization term (assuming L2 regularization)\n",
    "    reg_term = (C / len(X_train)) * (np.sum(W**2) + np.sum(T**2))\n",
    "    return -total_log_p / len(X_train) + reg_term\n",
    "\n",
    "def run_sgd(X_train, y_train, X_test, y_test, C, num_iterations, batch_size, learning_rate, momentum_coeff, use_momentum=False):\n",
    "    global params, velocity\n",
    "    train_objectives = []\n",
    "    test_errors = []\n",
    "\n",
    "    # Initialize params and velocity if not already initialized\n",
    "    if params.size == 0:\n",
    "        params = np.zeros((26*128+26*26,))\n",
    "    if velocity.size == 0:\n",
    "        velocity = np.zeros((26*128+26*26,))\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # Randomly sample indices for this iteration\n",
    "        indices = np.random.choice(len(X_train), size=batch_size, replace=False)\n",
    "        data_batch = [(X_train[i], y_train[i]) for i in indices]  # This is a list of tuples\n",
    "\n",
    "        # Perform a single SGD step\n",
    "        if use_momentum:\n",
    "            params, velocity = sgd_momentum_step(params, velocity, data_batch, C, learning_rate, momentum_coeff)\n",
    "        else:\n",
    "            params = sgd_step(params, data_batch, C, learning_rate)\n",
    "\n",
    "        # Compute training objective and test error after this step\n",
    "        W, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "        train_objective = compute_training_objective(W, T, X_train, y_train, C)\n",
    "        test_error = compute_test_error(W, T, X_test, y_test)\n",
    "\n",
    "        train_objectives.append(train_objective)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    return train_objectives, test_errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6ec69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 3438\n",
      "First 5 sequences' labels:\n",
      " [array([ 0, 10,  4]), array([14, 12, 12,  0, 13,  3,  8, 13,  6]), array([ 4, 17, 14]), array([13,  4, 23, 15,  4,  2, 19,  4,  3]), array([ 4,  2, 11,  0, 17,  8, 13,  6])]\n",
      "Shapes of model data:\n",
      "W: (26, 128) T: (26, 26)\n",
      "Number of training sequences: 3438\n",
      "First 5 sequences' labels:\n",
      " [array([ 0, 10,  4]), array([14, 12, 12,  0, 13,  3,  8, 13,  6]), array([ 4, 17, 14]), array([13,  4, 23, 15,  4,  2, 19,  4,  3]), array([ 4,  2, 11,  0, 17,  8, 13,  6])]\n",
      "Number of test sequences: 3439\n",
      "First 5 sequences' labels:\n",
      " [array([24, 11, 14, 15,  7, 14, 13,  4]), array([13, 22, 14, 17, 10,  0,  1, 11,  4]), array([ 2,  2, 14, 20, 13, 19,  0,  1,  8, 11,  8, 19, 24]), array([17,  8,  6,  7, 19,  5, 20, 11, 11, 24]), array([ 4,  2, 14, 12, 15, 17,  4, 18, 18])]\n"
     ]
    }
   ],
   "source": [
    "# For example:\n",
    "C = 1000  # Replace with the actual value\n",
    "learning_rate = 0.01  # Replace with the actual value\n",
    "momentum_coeff = 0.9  # Replace with the actual value\n",
    "batch_size = 10  # Replace with the actual value\n",
    "num_iterations = 100  # Replace with the actual value\n",
    "\n",
    "\n",
    "# Assuming that read_train and read_model are available\n",
    "X_y = read_train()\n",
    "W, T = read_model()\n",
    "data = X_y\n",
    "\n",
    "# Assuming parameter_shape is 26*128+26*26 based on your func_prime implementation\n",
    "params = np.concatenate([W.flatten(), T.flatten()])  # Concatenate W and T into a single parameter vector\n",
    "velocity = np.zeros_like(params)\n",
    "velocity = np.zeros(parameter_shape)\n",
    "\n",
    "# Call the functions\n",
    "train_data = read_train()\n",
    "test_data = read_test()\n",
    "\n",
    "# Unpack the feature matrices and labels vectors from the list of tuples\n",
    "X_train = [x for x, _ in train_data]\n",
    "y_train = [y for _, y in train_data]\n",
    "\n",
    "X_test = [x for x, _ in test_data]\n",
    "y_test = [y for _, y in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8fcfa7b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sgd_step() missing 1 required positional argument: 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run SGD with the updated compute_test_error function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_objectives, test_errors \u001b[38;5;241m=\u001b[39m run_sgd(X_train, y_train, X_test, y_test, C, num_iterations, batch_size, learning_rate, momentum_coeff, use_momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[89], line 77\u001b[0m, in \u001b[0;36mrun_sgd\u001b[1;34m(X_train, y_train, X_test, y_test, C, num_iterations, batch_size, learning_rate, momentum_coeff, use_momentum)\u001b[0m\n\u001b[0;32m     75\u001b[0m     params, velocity \u001b[38;5;241m=\u001b[39m sgd_momentum_step(params, velocity, data_batch, C, learning_rate, momentum_coeff)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     params \u001b[38;5;241m=\u001b[39m sgd_step(params, data_batch, C, learning_rate)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Compute training objective and test error after this step\u001b[39;00m\n\u001b[0;32m     80\u001b[0m W, T \u001b[38;5;241m=\u001b[39m params[:\u001b[38;5;241m26\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m128\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m26\u001b[39m, \u001b[38;5;241m128\u001b[39m)), params[\u001b[38;5;241m26\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m128\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m26\u001b[39m, \u001b[38;5;241m26\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: sgd_step() missing 1 required positional argument: 'learning_rate'"
     ]
    }
   ],
   "source": [
    "# Run SGD with the updated compute_test_error function\n",
    "train_objectives, test_errors = run_sgd(X_train, y_train, X_test, y_test, C, num_iterations, batch_size, learning_rate, momentum_coeff, use_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming compute_training_objective and compute_test_error are available\n",
    "# Assuming X_train, y_train, X_test, y_test are loaded and available\n",
    "# Assuming parameter_shape is known and appropriate for the CRF model\n",
    "# Assuming gradient_func and objective_func are implemented for the CRF model\n",
    "# The learning_rate, momentum_coeff, batch_size, num_iterations, n, and C should be appropriately set\n",
    "\n",
    "# Initialize parameters for the CRF model\n",
    "params = np.zeros(parameter_shape)\n",
    "velocity = np.zeros(parameter_shape)\n",
    "\n",
    "# Initialize lists to store the progress of training\n",
    "train_objectives_sgd = []\n",
    "train_objectives_momentum = []\n",
    "test_errors_sgd = []\n",
    "test_errors_momentum = []\n",
    "\n",
    "# Function for stochastic gradient descent with and without momentum\n",
    "def run_sgd(X_train, y_train, X_test, y_test, use_momentum=False):\n",
    "    global params, velocity\n",
    "    train_objectives = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Stochastic mini-batch sampling\n",
    "        indices = np.random.choice(len(X_train), size=batch_size, replace=False)\n",
    "        X_batch, y_batch = X_train[indices], y_train[indices]\n",
    "\n",
    "        # Compute the gradient using the sampled mini-batch\n",
    "        gradient = gradient_func(params, X_batch, y_batch, C)\n",
    "        \n",
    "        # Update parameters\n",
    "        if use_momentum:\n",
    "            # Update rule for SGD with momentum\n",
    "            velocity = momentum_coeff * velocity - learning_rate * gradient\n",
    "            params += velocity\n",
    "        else:\n",
    "            # Update rule for simple SGD\n",
    "            params -= learning_rate * gradient\n",
    "        \n",
    "        # Compute training objective value and test error\n",
    "        train_objective = objective_func(params, X_train, y_train, C)\n",
    "        test_error = compute_test_error(params, X_test, y_test)\n",
    "        \n",
    "        # Store the computed values\n",
    "        train_objectives.append(train_objective)\n",
    "        test_errors.append(test_error)\n",
    "    \n",
    "    return train_objectives, test_errors\n",
    "\n",
    "# Run SGD and SGD with momentum\n",
    "train_objectives_sgd, test_errors_sgd = run_sgd(X_train, y_train, X_test, y_test, use_momentum=False)\n",
    "train_objectives_momentum, test_errors_momentum = run_sgd(X_train, y_train, X_test, y_test, use_momentum=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1977922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training objective decline\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, train_objectives_sgd, label='SGD Training Objective')\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, train_objectives_momentum, label='SGD with Momentum Training Objective')\n",
    "plt.xlabel('Effective number of passes')\n",
    "plt.ylabel('Training Objective Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot test error decline\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, test_errors_sgd, label='SGD Test Error')\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, test_errors_momentum, label='SGD with Momentum Test Error')\n",
    "plt.xlabel('Effective number of passes')\n",
    "plt.ylabel('Word-wise Test Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For LBFGS, you would use scipy.optimize.fmin_tnc or similar functions\n",
    "# You would also implement a callback function similar to the one described earlier\n",
    "# The callback would need to track function evaluations, test errors, and training objectives\n",
    "# The plots for LBFGS would need to be added to the above plots for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
