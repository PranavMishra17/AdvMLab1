{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60047677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c29bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of decode input:\n",
      "X: (100, 128) W: (26, 128) T: (26, 26)\n",
      "Shapes of train_struct:\n",
      "dataX: (25953, 128) dataY length: 25953\n",
      "Shapes of model data:\n",
      "W: (26, 128) T: (26, 26)\n",
      "Number of training sequences: 3438\n",
      "First 5 sequences' labels:\n",
      " [array([ 0, 10,  4]), array([14, 12, 12,  0, 13,  3,  8, 13,  6]), array([ 4, 17, 14]), array([13,  4, 23, 15,  4,  2, 19,  4,  3]), array([ 4,  2, 11,  0, 17,  8, 13,  6])]\n",
      "Number of test sequences: 3439\n",
      "First 5 sequences' labels:\n",
      " [array([24, 11, 14, 15,  7, 14, 13,  4]), array([13, 22, 14, 17, 10,  0,  1, 11,  4]), array([ 2,  2, 14, 20, 13, 19,  0,  1,  8, 11,  8, 19, 24]), array([17,  8,  6,  7, 19,  5, 20, 11, 11, 24]), array([ 4,  2, 14, 12, 15, 17,  4, 18, 18])]\n",
      "Shape of test data for decoder: (26198, 128)\n",
      "Top 5 feature vectors:\n",
      " [[0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      "  0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      "  0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      "  1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      "  1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      "  0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      "  1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define file paths for ease of access\n",
    "decode_input_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/decode_input.txt\"\n",
    "train_struct_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/train_struct.txt\"\n",
    "model_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/model.txt\"\n",
    "train_data_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/train.txt\"\n",
    "test_data_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/test.txt\"\n",
    "\n",
    "def read_decode_input(file_path):\n",
    "    \"\"\"\n",
    "    Reads the decode_input data from the file.\n",
    "    Each line represents one letter with 128 elements.\n",
    "    There are 26 weight vectors each with 128 elements and a transition matrix T with size 26x26.\n",
    "    The transition matrix T is in row-major order.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    X = np.array(raw_data[:100 * 128], dtype=float).reshape(100, 128)\n",
    "    W = np.array(raw_data[100 * 128:100 * 128 + 26 * 128], dtype=float).reshape(26, 128)\n",
    "    T = np.array(raw_data[100 * 128 + 26 * 128:-1], dtype=float).reshape(26, 26)\n",
    "    T = np.swapaxes(T, 0, 1)\n",
    "    \n",
    "    print(\"Shapes of decode input:\")\n",
    "    print(\"X:\", X.shape, \"W:\", W.shape, \"T:\", T.shape)\n",
    "    #print(\"Top 5 rows of X:\\n\", X[:5])\n",
    "\n",
    "    return X, W, T\n",
    "\n",
    "def read_train_struct(file_path):\n",
    "    \"\"\"\n",
    "    Reads the train_struct data from the file.\n",
    "    Each line represents a label and a feature vector (in a sparse representation).\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    for line in raw_data[:-1]:  # The last element is empty\n",
    "        line = line.split(\" \")\n",
    "        dataY.append([int(line[0]) - 1, int(line[1][4:])])\n",
    "        datax = np.zeros(128, dtype=int)\n",
    "        for f1 in line[2:]:\n",
    "            idx, val = f1.split(\":\")\n",
    "            datax[int(idx) - 1] = int(val)\n",
    "        dataX.append(datax)\n",
    "    \n",
    "    dataX_np = np.array(dataX, dtype=int)\n",
    "    print(\"Shapes of train_struct:\")\n",
    "    print(\"dataX:\", dataX_np.shape, \"dataY length:\", len(dataY))\n",
    "    #print(\"Top 5 rows of dataX:\\n\", dataX_np[:5])\n",
    "    \n",
    "    return dataX_np, dataY\n",
    "\n",
    "def read_model(file_path):\n",
    "    \"\"\"\n",
    "    Reads the model data from the file.\n",
    "    The data consists of weight vectors for each label and a transition matrix T.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    W = np.array(raw_data[:26 * 128], dtype=float).reshape(26, 128)\n",
    "    T = np.array(raw_data[26 * 128:-1], dtype=float).reshape(26, 26)\n",
    "    T = np.swapaxes(T, 0, 1)\n",
    "    \n",
    "    print(\"Shapes of model data:\")\n",
    "    print(\"W:\", W.shape, \"T:\", T.shape)\n",
    "    #print(\"Top 5 rows of W:\\n\", W[:5])\n",
    "    \n",
    "    return W, T\n",
    "\n",
    "def read_train(file_path):\n",
    "    \"\"\"\n",
    "    Reads the train data from the file.\n",
    "    Each row corresponds to an example and is split into the label and the feature vector.\n",
    "    \"\"\"\n",
    "    from string import ascii_lowercase\n",
    "    mapping = {letter: idx for idx, letter in enumerate(ascii_lowercase)}\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:  # End of sequence\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []  # Reset for the next sequence\n",
    "\n",
    "    print(\"Number of training sequences:\", len(dataX))\n",
    "    print(\"First 5 sequences' labels:\\n\", dataY[:5])\n",
    "    \n",
    "    return list(zip(dataX, dataY))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_test(file_path):\n",
    "    \"\"\"\n",
    "    Reads the test data from the file.\n",
    "    Each row corresponds to an example and is split into the label and the feature vector.\n",
    "    The function assumes that each example ends when a row with the third column less than 0 is encountered.\n",
    "    \"\"\"\n",
    "    from string import ascii_lowercase\n",
    "    mapping = {letter: idx for idx, letter in enumerate(ascii_lowercase)}\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:  # Skip the last empty line if it exists\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:  # Check for the end of a sequence\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []  # Reset for the next sequence\n",
    "\n",
    "    print(\"Number of test sequences:\", len(dataX))\n",
    "    print(\"First 5 sequences' labels:\\n\", dataY[:5])\n",
    "\n",
    "    return list(zip(dataX, dataY))\n",
    "\n",
    "def read_test_decoder_modified(file_path):\n",
    "    \"\"\"\n",
    "    Reads the test data for decoding and returns a NumPy array\n",
    "    where each sub-array from the list becomes a row in the final\n",
    "    two-dimensional array. This function only extracts the features\n",
    "    and does not deal with the labels.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        raw_data = file.read().strip().split('\\n')\n",
    "\n",
    "    # Initialize an empty list to store all feature vectors\n",
    "    dataX = []\n",
    "    \n",
    "    for row in raw_data:\n",
    "        if row:  # Skip any empty lines\n",
    "            features = row.split(' ')[5:]  # Features start from the 6th element in the row\n",
    "            feature_vector = list(map(float, features))  # Convert string features to float\n",
    "            dataX.append(feature_vector)\n",
    "\n",
    "    # Convert the list of lists (features for each word) into a 2D NumPy array\n",
    "    dataX_np = np.array(dataX)\n",
    "\n",
    "    print(\"Shape of test data for decoder:\", dataX_np.shape)\n",
    "    print(\"Top 5 feature vectors:\\n\", dataX_np[:5, :])\n",
    "\n",
    "    return dataX_np\n",
    "\n",
    "\n",
    "# Example:\n",
    "X, W, T = read_decode_input(decode_input_path)\n",
    "train_struct_X, train_struct_Y = read_train_struct(train_struct_path)\n",
    "W_model, T_model = read_model(model_path)\n",
    "train_data = read_train(train_data_path)\n",
    "test_data = read_test(test_data_path)\n",
    "test_data_decoder = read_test_decoder_modified(test_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5ec39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Defining paths for the required files\n",
    "path_decode_input = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/decode_input.txt\"\n",
    "path_train_struct = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/train_struct.txt\"\n",
    "path_train = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/train.txt\"\n",
    "path_test = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/test_struct.txt\"\n",
    "\n",
    "def read_parameters():\n",
    "    \"\"\"\n",
    "    Reads the decode_input file and extracts X, W, and T parameters.\n",
    "    X: Feature vectors\n",
    "    W: Weight vectors\n",
    "    T: Transition matrix\n",
    "    \"\"\"\n",
    "    raw_data = np.loadtxt(path_decode_input, ndmin=1)\n",
    "    \n",
    "    # Splitting raw_data into respective parts for X, W, T\n",
    "    X = np.array(raw_data[0:100*128]).reshape(100,128)\n",
    "    W = np.array(raw_data[100*128:100*128+26*128]).reshape(26,128)\n",
    "    T = np.array(raw_data[100*128+26*128:]).reshape(26,26)\n",
    "    \n",
    "    # Debug: Print shapes to ensure correct reshaping\n",
    "    print(f\"Shapes - X: {X.shape}, W: {W.shape}, T: {T.shape}\")\n",
    "    return X, W, T\n",
    "\n",
    "def read_word_indexes():\n",
    "    \"\"\"\n",
    "    Reads the train.txt file to extract the word indexes.\n",
    "    \"\"\"\n",
    "    # Using np.loadtxt to only load specific column\n",
    "    word_indexes = np.loadtxt(path_train, usecols=(2,))\n",
    "    \n",
    "    # Debug: Print first 5 word indexes to check correctness\n",
    "    print(\"First 5 word indexes:\", word_indexes[:5])\n",
    "    return word_indexes\n",
    "\n",
    "def read_train_struct():\n",
    "    \"\"\"\n",
    "    Reads the train_struct.txt file to extract training data X and labels Y.\n",
    "    \"\"\"\n",
    "    with open(path_train_struct, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    for line in raw_data[:-1]:\n",
    "        line = line.split(\" \")\n",
    "        dataY.append([int(line[0])-1]) # Assuming labels are to be decremented by 1\n",
    "        datax = [0]*128\n",
    "        for feature in line[2:]:\n",
    "            index, _ = feature.split(\":\")\n",
    "            datax[int(index)-1] = 1  # Set the corresponding feature to 1\n",
    "        dataX.append(datax)\n",
    "    \n",
    "    dataX, dataY = np.array(dataX, dtype=float), np.array(dataY, dtype=int)\n",
    "    \n",
    "    # Debug: Print shapes and first few labels for verification\n",
    "    print(f\"dataX shape: {dataX.shape}, dataY shape: {dataY.shape}\")\n",
    "    print(f\"First 5 labels: {dataY[:5]}\")\n",
    "    return dataX, dataY\n",
    "\n",
    "def read_test_struct():\n",
    "    \"\"\"\n",
    "    Reads the test_struct.txt file to extract test data X and labels Y.\n",
    "    \"\"\"\n",
    "    with open(path_test, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    for line in raw_data[:-1]:\n",
    "        line = line.split(\" \")\n",
    "        dataY.append([int(line[0])-1])  # Assuming labels are to be decremented by 1\n",
    "        datax = [0]*128\n",
    "        for feature in line[2:]:\n",
    "            index, _ = feature.split(\":\")\n",
    "            datax[int(index)-1] = 1  # Set the corresponding feature to 1\n",
    "        dataX.append(datax)\n",
    "    \n",
    "    dataX, dataY = np.array(dataX, dtype=float), np.array(dataY, dtype=int)\n",
    "    \n",
    "    # Debug: Print shapes and first few labels for verification\n",
    "    print(f\"dataX shape: {dataX.shape}, dataY shape: {dataY.shape}\")\n",
    "    print(f\"First 5 labels: {dataY[:5]}\")\n",
    "    return dataX, dataY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2702b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - X: (100, 128), W: (26, 128), T: (26, 26)\n",
      "First 5 word indexes: [ 2.  3. -1.  2.  3.]\n",
      "Word indexes (First 10): [ 2.  3. -1.  2.  3.  4.  5.  6.  7.  8.] \n",
      "\n",
      "dataX shape: (25953, 128), dataY shape: (25953, 1)\n",
      "First 5 labels: [[ 0]\n",
      " [10]\n",
      " [ 4]\n",
      " [14]\n",
      " [12]]\n",
      "Training dataX (First 5 examples): [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      "  1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      "  1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
      "  0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      "  0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      "  0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      "  1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      "  0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      "  1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      "  1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      "Training dataY (First 5 labels): [[ 0]\n",
      " [10]\n",
      " [ 4]\n",
      " [14]\n",
      " [12]] \n",
      "\n",
      "dataX shape: (26198, 128), dataY shape: (26198, 1)\n",
      "First 5 labels: [[24]\n",
      " [11]\n",
      " [14]\n",
      " [15]\n",
      " [ 7]]\n",
      "Test dataX (First 5 examples): [[0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      "  0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      "  0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      "  1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      "  1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      "  0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      "  0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      "  1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 1. 0.]] \n",
      "\n",
      "Test dataY (First 5 labels): [[24]\n",
      " [11]\n",
      " [14]\n",
      " [15]\n",
      " [ 7]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage for read_parameters\n",
    "X, W, T = read_parameters()\n",
    "#print(\"X (Feature vectors):\", X[:5], \"\\n\")  # Print first 5 rows for brevity\n",
    "#print(\"W (Weight vectors):\", W[:5], \"\\n\")  # Print first 5 rows for brevity\n",
    "#print(\"T (Transition matrix):\", T, \"\\n\")\n",
    "\n",
    "# Usage for read_word_indexes\n",
    "word_indexes = read_word_indexes()\n",
    "print(\"Word indexes (First 10):\", word_indexes[:10], \"\\n\")  # Print first 10 word indexes\n",
    "\n",
    "# Usage for read_train_struct\n",
    "dataX_train, dataY_train = read_train_struct()\n",
    "print(\"Training dataX (First 5 examples):\", dataX_train[:5], \"\\n\")  # Print first 5 examples for brevity\n",
    "print(\"Training dataY (First 5 labels):\", dataY_train[:5], \"\\n\")  # Print first 5 labels\n",
    "\n",
    "# Usage for read_test_struct\n",
    "dataX_test, dataY_test = read_test_struct()\n",
    "print(\"Test dataX (First 5 examples):\", dataX_test[:5], \"\\n\")  # Print first 5 examples for brevity\n",
    "print(\"Test dataY (First 5 labels):\", dataY_test[:5], \"\\n\")  # Print first 5 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3685d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 758.875146\n",
      "         Iterations: 20\n",
      "         Function evaluations: 22\n",
      "         Gradient evaluations: 22\n",
      "Time: 144.85742807388306\n"
     ]
    }
   ],
   "source": [
    "import numpy, data_read, prob_grad\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import time\n",
    "import pickle\n",
    "X_y = data_read.read_train()\n",
    "W, T = data_read.read_model()\n",
    "\n",
    "def func(params, *args):\n",
    "#objective function specified in the handout\n",
    "\tW, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "\tdata = args[0]\n",
    "\tC = args[1]\n",
    "\n",
    "\tlog_sum = 0\n",
    "\tfor example in data:\n",
    "\t\tlog_sum += prob_grad.compute_log_p(example[0], example[1], W, T)\n",
    "\n",
    "\tnorm = numpy.zeros(26)\n",
    "\tfor i in range(26):\n",
    "\t\tnorm[i] = numpy.linalg.norm(W[i])\n",
    "\n",
    "\tnumpy.square(norm, out=norm)\n",
    "\t\n",
    "\treturn -1*(C/len(data))*log_sum + 0.5*numpy.sum(norm) + 0.5*numpy.sum(numpy.square(T))\n",
    "\n",
    "def func_prime(params, *args):\n",
    "#derivative of objective function specified in the handout\n",
    "\tW, T = params[:26*128].reshape((26, 128)), params[26*128:].reshape((26, 26))\n",
    "\tdata = args[0]\n",
    "\tC = args[1]\n",
    "\n",
    "\tlog_grad_w = numpy.zeros((26, 128))\n",
    "\tlog_grad_t = numpy.zeros((26, 26))\n",
    "\n",
    "\t#gradient of logP w/ W\n",
    "\tfor example in data:\n",
    "\t\tnumpy.add(log_grad_w, prob_grad.log_p_wgrad(W,\\\n",
    "\t\t\texample[0], example[1], T), out=log_grad_w)\n",
    "\t\tnumpy.add(log_grad_t, prob_grad.log_p_tgrad(T,\\\n",
    "\t\t\texample[0], example[1], W), out=log_grad_t)\n",
    "\n",
    "\t#multiply C/N\n",
    "\tnumpy.multiply(log_grad_w, -1*C/len(data), out=log_grad_w)\n",
    "\tnumpy.multiply(log_grad_t, -1*C/len(data), out=log_grad_t)\n",
    "\n",
    "\t#add gradient of norm\n",
    "\tnumpy.add(log_grad_w, W, out=log_grad_w)\n",
    "\n",
    "\t#add normalizing factor\n",
    "\tnumpy.add(log_grad_t, T, out=log_grad_t)\n",
    "\n",
    "\treturn numpy.concatenate([log_grad_w.reshape(26*128),\\\n",
    "\t\tlog_grad_t.reshape(26*26)])\n",
    "\n",
    "#on = numpy.concatenate([W.reshape(26*128), T.reshape(26*26)])\n",
    "\n",
    "#res = func(on, X_y[:9], 1000)\n",
    "#result = func_prime(on, X_y[:9], 1000)\n",
    "\n",
    "#need to flatten for the optimizer\n",
    "initial_guess = numpy.zeros((26*128+26*26))\n",
    "\n",
    "#bounds = [(-10000000, 10000000)]*(28*128+26*26)\n",
    "\n",
    "\n",
    "#ret = fmin_bfgs(func, initial_guess, fprime=func_prime, args=(X_y[:5], 1000),\\\n",
    "#\tmaxiter=2, retall=True, full_output=True)\n",
    "\n",
    "def get_params(x_y):\n",
    "    t0 = time.time()\n",
    "    ret=fmin_bfgs(func, initial_guess, fprime=func_prime, args=(x_y,100),\\\n",
    "    \t maxiter=20,retall=True, full_output=True)\n",
    "    t1 = time.time()\n",
    "    with open(\"best_Weights_c100\",\"+bw\") as f :\n",
    "        pickle.dump(ret,f)\n",
    "    numpy.savetxt(\"best_Weights_c100\",ret[0])\n",
    "    #numpy.savetxt(\"best_func_c_10\",ret[1])\n",
    "    \n",
    "    print(f\"Time: {t1-t0}\")\n",
    "    \n",
    "get_params(X_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "881a3e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sequences: 3439\n",
      "First 5 sequences' labels:\n",
      " [array([24, 11, 14, 15,  7, 14, 13,  4]), array([13, 22, 14, 17, 10,  0,  1, 11,  4]), array([ 2,  2, 14, 20, 13, 19,  0,  1,  8, 11,  8, 19, 24]), array([17,  8,  6,  7, 19,  5, 20, 11, 11, 24]), array([ 4,  2, 14, 12, 15, 17,  4, 18, 18])]\n",
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def calculate_accuracy(prediction_file, test_file):\n",
    "    predictions = read_file(prediction_file)\n",
    "    test_data = read_test(test_file)\n",
    "    \n",
    "    total_samples = len(predictions)\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for pred_label, (test_X, test_Y) in zip(predictions, test_data):\n",
    "        test_label = test_Y[0]  # Assuming test_Y contains only one label per sequence\n",
    "        if pred_label == test_label:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    accuracy = (correct_predictions / total_samples) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "prediction_file = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/result/prediction.txt\"\n",
    "test_file = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/test.txt\"\n",
    "accuracy = calculate_accuracy(prediction_file, test_file)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd25d025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7afb7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sequences: 3439\n",
      "First 5 sequences' labels:\n",
      " [array([24, 11, 14, 15,  7, 14, 13,  4]), array([13, 22, 14, 17, 10,  0,  1, 11,  4]), array([ 2,  2, 14, 20, 13, 19,  0,  1,  8, 11,  8, 19, 24]), array([17,  8,  6,  7, 19,  5, 20, 11, 11, 24]), array([ 4,  2, 14, 12, 15, 17,  4, 18, 18])]\n",
      "Accuracy: 0.00809222078021223\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(predictions_path, ground_truth_path):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the predictions.\n",
    "    \n",
    "    :param predictions_path: Path to the predictions file.\n",
    "    :param ground_truth_path: Path to the ground truth labels file.\n",
    "    :return: Accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    # Use the read_test function and only extract the ground truth labels part\n",
    "    test_data = read_test(ground_truth_path)\n",
    "    ground_truth_labels = [label for _, labels in test_data for label in labels]\n",
    "\n",
    "    # Read predictions\n",
    "    with open(predictions_path, 'r') as f:\n",
    "        predictions = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "    # Ensure the total number of labels matches the number of predictions\n",
    "    if len(predictions) != len(ground_truth_labels):\n",
    "        raise ValueError(\"The number of predictions does not match the number of ground truth labels.\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_predictions = sum(p == gt for p, gt in zip(predictions, ground_truth_labels))\n",
    "    accuracy = correct_predictions / len(ground_truth_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "predictions_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/result/prediction.txt\"\n",
    "ground_truth_path = \"C:/Users/prana/Desktop/Fall 23/Adv ML/LAB1/CRF-master/data/test.txt\"\n",
    "accuracy = calculate_accuracy(predictions_path, ground_truth_path)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344452f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
