{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6fbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# This is prob_grad.py\n",
    "\n",
    "def compute_log_p(X, y, W, T):\n",
    "    \"\"\"\n",
    "    Computes the log probability of a sequence of labels given inputs X and parameters W, T.\n",
    "    \n",
    "    Parameters:\n",
    "    X : 2D array where each row is the feature vector for one observation.\n",
    "    y : 1D array of labels corresponding to the observations in X.\n",
    "    W : Weight matrix where each row corresponds to the weights for one label.\n",
    "    T : Transition matrix where T[i, j] is the transition weight from label i to label j.\n",
    "    \n",
    "    Returns:\n",
    "    log probability of the label sequence given the inputs and parameters.\n",
    "    \"\"\"\n",
    "    # Initialize the log probability sum\n",
    "    sum_num = np.dot(W[y[0]], X[0])  # Contribution from the first label and feature\n",
    "    \n",
    "    # Add contributions from the rest of the labels and transitions\n",
    "    for i in range(1, X.shape[0]):\n",
    "        sum_num += np.dot(W[y[i]], X[i]) + T[y[i-1], y[i]]\n",
    "    \n",
    "    # Compute forward probabilities for log partition function (normalizing constant)\n",
    "    alpha_len = W.shape[0]  # Number of possible labels (alphabet size)\n",
    "    trellisfw = np.zeros((X.shape[0], alpha_len))\n",
    "    for i in range(1, X.shape[0]):\n",
    "        np.matmul(W, X[i-1], out=trellisfw[i-1])\n",
    "        T_ext = T + trellisfw[i-1][:, np.newaxis]  # Add the transition scores\n",
    "        log_sum = np.log(np.sum(np.exp(T_ext - np.max(T_ext)), axis=0))\n",
    "        trellisfw[i] = log_sum + np.max(T_ext)\n",
    "\n",
    "    # Log partition function is the log sum of the last column of forward probabilities\n",
    "    log_z = np.max(trellisfw[-1]) + np.log(np.sum(np.exp(trellisfw[-1] - np.max(trellisfw[-1]))))\n",
    "    \n",
    "    # Return log probability of the sequence\n",
    "    return sum_num - log_z\n",
    "\n",
    "def fb_prob(X, W, T):\n",
    "    \"\"\"\n",
    "    Computes forward and backward probabilities for all labels given inputs X and parameters W, T.\n",
    "    \n",
    "    Parameters:\n",
    "    X : 2D array where each row is the feature vector for one observation.\n",
    "    W : Weight matrix where each row corresponds to the weights for one label.\n",
    "    T : Transition matrix where T[i, j] is the transition weight from label i to label j.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of (forward probabilities, backward probabilities, log partition function)\n",
    "    \"\"\"\n",
    "    alpha_len = 26  # Number of labels = W.shape[0]\n",
    "    trellisfw = np.zeros((X.shape[0], alpha_len))  # Forward probabilities\n",
    "    trellisbw = np.zeros((X.shape[0], alpha_len))  # Backward probabilities\n",
    "\n",
    "    # Forward pass\n",
    "    for i in range(1, X.shape[0]):\n",
    "        \n",
    "        # Matrix multiplication of W and X[i-1] (input features)\n",
    "        np.matmul(W, X[i-1], out=trellisfw[i-1])\n",
    "        \n",
    "        # Adding transition scores\n",
    "        np.add(T, trellisfw[i-1][:, np.newaxis], out=trellisfw[i])\n",
    "        \n",
    "        # Log-sum-exp trick for numerical stability\n",
    "        max_val = np.max(trellisfw[i], axis=0)\n",
    "        np.subtract(trellisfw[i], max_val, out=trellisfw[i])\n",
    "        np.exp(trellisfw[i], out=trellisfw[i])\n",
    "        sum_exp = np.sum(trellisfw[i], axis=0)\n",
    "        log_sum_exp = np.log(sum_exp)\n",
    "        trellisfw[i] = log_sum_exp + max_val\n",
    "\n",
    "    # Debug: print the shape of the forward trellis\n",
    "    print(\"Forward trellis shape:\", trellisfw.shape)\n",
    "\n",
    "    # Backward pass (compute backward probabilities)\n",
    "    trellisbw[-1, :] = 0  # Initialize the last row of the backward trellis with zeros (log(1))\n",
    "    for i in range(X.shape[0] - 2, -1, -1):\n",
    "        # Compute the weighted features for the next time step\n",
    "        weighted_features = np.dot(W, X[i + 1])\n",
    "        # Compute the backward messages for each label\n",
    "        for label in range(alpha_len):\n",
    "            trellisbw[i, label] = np.log(np.sum(\n",
    "                np.exp(\n",
    "                    T[label, :] + trellisbw[i + 1, :] + weighted_features - log_z\n",
    "                )\n",
    "            ))\n",
    "\n",
    "    # Debug: print the shape of the backward trellis\n",
    "    print(\"Backward trellis shape:\", trellisbw.shape)\n",
    "\n",
    "    # Log partition function (computed using the forward trellis)\n",
    "    log_z = np.max(trellisfw[-1]) + np.log(np.sum(np.exp(trellisfw[-1] - np.max(trellisfw[-1]))))\n",
    "\n",
    "    # Debug: print the value of the log partition function\n",
    "    print(\"Log partition function:\", log_z)\n",
    "\n",
    "    return trellisfw, trellisbw, log_z\n",
    "\n",
    "# The following functions compute gradients for the weight matrix W and transition matrix T respectively\n",
    "# given a single example (X, y), where X is the feature matrix for the sequence and y is the corresponding label sequence.\n",
    "\n",
    "def log_p_wgrad(W, X, y, T):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the log probability with respect to the weight matrix W.\n",
    "    \n",
    "    Parameters:\n",
    "    W : Weight matrix where each row corresponds to the weights for one label.\n",
    "    X : 2D array where each row is the feature vector for one observation.\n",
    "    y : 1D array of labels corresponding to the observations in X.\n",
    "    T : Transition matrix where T[i, j] is the transition weight from label i to label j.\n",
    "    \n",
    "    Returns:\n",
    "    Gradient of the log probability with respect to W.\n",
    "    \"\"\"\n",
    "    grad_W = np.zeros(W.shape)  # Gradient matrix for W\n",
    "    trellisfw, trellisbw, log_z = fb_prob(X, W, T)\n",
    "\n",
    "    # Iterate over the sequence\n",
    "    for i in range(X.shape[0]):\n",
    "        # Combine forward and backward messages\n",
    "        marginal = trellisfw[i] + trellisbw[i]\n",
    "        # Incorporate the evidence from input features\n",
    "        evidence = np.matmul(W, X[i])\n",
    "        # Subtract the log partition function\n",
    "        marginal -= log_z\n",
    "        # Normalize to get probabilities\n",
    "        marginal = np.exp(marginal)\n",
    "\n",
    "        # Calculate the gradient for the current position\n",
    "        for j in range(26):  # Iterate over all possible labels\n",
    "            if j == y[i]:\n",
    "                grad_W[j] += X[i]  # Add the feature vector for the true label\n",
    "            grad_W[j] -= marginal[j] * X[i]  # Subtract the expected feature vector\n",
    "\n",
    "    return grad_W\n",
    "\n",
    "def log_p_tgrad(T, X, y, W):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the log probability with respect to the transition matrix T.\n",
    "    \n",
    "    Parameters:\n",
    "    T : Transition matrix where T[i, j] is the transition weight from label i to label j.\n",
    "    X : 2D array where each row is the feature vector for one observation.\n",
    "    y : 1D array of labels corresponding to the observations in X.\n",
    "    W : Weight matrix where each row corresponds to the weights for one label.\n",
    "    \n",
    "    Returns:\n",
    "    Gradient of the log probability with respect to T.\n",
    "    \"\"\"\n",
    "    grad_T = np.zeros(T.shape)  # Gradient matrix for T\n",
    "    trellisfw, trellisbw, log_z = fb_prob(X, W, T)\n",
    "\n",
    "    # Iterate over the transitions in the sequence\n",
    "    for i in range(X.shape[0] - 1):\n",
    "        # Calculate the potential for all label transitions\n",
    "        potential = np.outer(np.matmul(W, X[i]), np.matmul(W, X[i + 1]))\n",
    "        # Add transition scores\n",
    "        potential += T\n",
    "        # Incorporate forward and backward messages\n",
    "        potential += trellisfw[i, :, np.newaxis] + trellisbw[i + 1]\n",
    "        # Subtract the log partition function\n",
    "        potential -= log_z\n",
    "        # Normalize to get joint probabilities of label transitions\n",
    "        potential = np.exp(potential)\n",
    "\n",
    "        # Calculate the gradient for the current transition\n",
    "        for j in range(26):\n",
    "            for k in range(26):\n",
    "                if j == y[i] and k == y[i + 1]:\n",
    "                    grad_T[j, k] += 1  # Increment for the true transition\n",
    "                grad_T[j, k] -= potential[j, k]  # Subtract the expected count\n",
    "\n",
    "    return grad_T\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X, y, W, and T are already loaded\n",
    "# grad_W = log_p_wgrad(W, X, y, T)\n",
    "# grad_T = log_p_tgrad(T, X, y, W)\n",
    "# The gradients can be used in an optimization algorithm to update W and T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4116d",
   "metadata": {},
   "source": [
    "In the backward pass, we go backwards through the time steps and calculate the backward probabilities, which, along with the forward probabilities, are used to compute the marginal probabilities. The **log_z** value is the log partition function computed from the forward trellis, which serves as the normalization constant to ensure that the probabilities sum to one.\n",
    "\n",
    "The above functions **log_p_wgrad** and **log_p_tgrad** compute the gradients of the log probability with respect to the weights matrix W and the transition matrix T, respectively. They utilize the **fb_prob** function, which performs the forward-backward algorithm to compute the necessary probabilities. Debugging statements and comprehensive comments have been added for clarity. Please ensure that you have the correct data structures and that the matrices W and T are properly initialized before calling these functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb94e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e3521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
