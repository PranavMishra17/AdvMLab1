{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac1e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6e4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37045e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'read_data' from 'data_read' (C:\\Users\\prana\\Desktop\\Fall 23\\Adv ML\\LAB1\\CRF-master\\code\\data_read.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_read\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_data \u001b[38;5;66;03m# Adapt function name as necessary\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mt_grad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_t_gradient\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mw_grad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_w_gradient\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'read_data' from 'data_read' (C:\\Users\\prana\\Desktop\\Fall 23\\Adv ML\\LAB1\\CRF-master\\code\\data_read.py)"
     ]
    }
   ],
   "source": [
    "from data_read import read_data # Adapt function name as necessary\n",
    "from t_grad import compute_t_gradient\n",
    "from w_grad import compute_w_gradient\n",
    "from prob_grad import compute_prob_gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1278671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Placeholder for the actual computation of objective and gradient for a batch\n",
    "def compute_objective_gradient(batch, C, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute the objective value and gradient for the given batch.\n",
    "    C: Regularization parameter\n",
    "    X_train, y_train: Training data and labels\n",
    "    \"\"\"\n",
    "    # This should compute the stochastic objective and gradient based on the batch\n",
    "    # For illustration, it returns random values\n",
    "    objective = np.random.rand()\n",
    "    gradient = np.random.randn(len(batch))\n",
    "    return objective, gradient\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train, C, n_iter, B, step_size):\n",
    "    n_train = len(X_train)\n",
    "    # Assuming `theta` initialization (adapt based on model specifics)\n",
    "    theta = np.zeros(X_train.shape[1]) # Modify based on actual model parameters\n",
    "    \n",
    "    objectives = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Sample a mini-batch\n",
    "        batch_indices = np.random.choice(n_train, B, replace=False)\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_y = y_train[batch_indices]\n",
    "        \n",
    "        # Compute gradients for the batch\n",
    "        t_gradient = compute_t_gradient(batch_X, batch_y, theta, C)\n",
    "        w_gradient = compute_w_gradient(batch_X, batch_y, theta, C)\n",
    "        # You can replace the above gradient computation with the appropriate function calls\n",
    "        \n",
    "        # Update rule; for simplicity, we're updating theta using t_gradient as an example\n",
    "        theta = theta - step_size * t_gradient\n",
    "        \n",
    "        # Recompute the objective value for plotting; use appropriate function if available\n",
    "        objective = compute_prob_gradient(batch_X, batch_y, theta, C) # Adapt as necessary\n",
    "        objectives.append(objective)\n",
    "        \n",
    "        print(f'>{i} Objective: {objective:.5f}')\n",
    "    \n",
    "    return theta, objectives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f0439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_momentum(C, X_train, y_train, n_iter, B, step_size, momentum):\n",
    "    n_train = len(X_train)\n",
    "    solutions, objectives = [], []\n",
    "    change = 0.0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Sample a mini-batch\n",
    "        batch_indices = np.random.choice(n_train, B, replace=False)\n",
    "        batch_X = X_train[batch_indices]\n",
    "        batch_y = y_train[batch_indices]\n",
    "        \n",
    "        # Compute objective and gradient for the batch\n",
    "        objective, gradient = compute_objective_gradient(batch_indices, C, batch_X, batch_y)\n",
    "        \n",
    "        # Update parameters with momentum\n",
    "        new_change = step_size * gradient + momentum * change\n",
    "        theta = theta - new_change\n",
    "        change = new_change\n",
    "        \n",
    "        # Store solution and objective value\n",
    "        solutions.append(theta)\n",
    "        objectives.append(objective)\n",
    "        \n",
    "        print(f'>{i} Objective: {objective:.5f}')\n",
    "    \n",
    "    return solutions, objectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3cd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running optimization\n",
    "plt.plot(objectives, label='SGD')\n",
    "plt.xlabel('Effective Number of Passes')\n",
    "plt.ylabel('Objective Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_with_momentum(parameters, gradients, velocity, learning_rate=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Performs a single update on the parameters using SGD with momentum.\n",
    "\n",
    "    :param parameters: Dictionary containing model parameters\n",
    "    :param gradients: Dictionary containing gradients of parameters\n",
    "    :param velocity: Dictionary containing the velocity of parameters\n",
    "    :param learning_rate: Learning rate for the update\n",
    "    :param momentum: Momentum coefficient\n",
    "    :return: Updated parameters and velocity\n",
    "    \"\"\"\n",
    "    for key in parameters.keys():\n",
    "        # Update the velocity\n",
    "        velocity[key] = momentum * velocity[key] + learning_rate * gradients[key]\n",
    "        # Update parameters\n",
    "        parameters[key] = parameters[key] - velocity[key]\n",
    "    \n",
    "    return parameters, velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eca8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback function\n",
    "def callback(params, *args):\n",
    "    # params will contain the current solution (weights w)\n",
    "    # args can contain other necessary data, like a counter for function evaluations\n",
    "    # Typically args will include X_test, y_test, objective function, etc.\n",
    "\n",
    "    # Increment the global variable counter\n",
    "    global function_evaluations\n",
    "    function_evaluations += 1\n",
    "\n",
    "    # Extract X_test and y_test from args if they're provided\n",
    "    X_test, y_test = args[0], args[1]\n",
    "\n",
    "    # Evaluate the word-wise test error for the current weights\n",
    "    test_error = compute_test_error(params, X_test, y_test)\n",
    "\n",
    "    # Recompute the current training objective value\n",
    "    training_objective = objective_func(params)\n",
    "\n",
    "    # Print or log the two numbers\n",
    "    print(f\"Function Evaluations: {function_evaluations}, Test Error: {test_error}, Training Objective: {training_objective}\")\n",
    "\n",
    "    # Optionally, save these values to a list for plotting later\n",
    "    test_errors.append(test_error)\n",
    "    training_objectives.append(training_objective)\n",
    "\n",
    "# Initialize a counter\n",
    "function_evaluations = 0\n",
    "\n",
    "# Initialize lists to store progress for plotting\n",
    "test_errors = []\n",
    "training_objectives = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0a8f0",
   "metadata": {},
   "source": [
    "# from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e2ec20",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_train_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataX, dataY)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Now let's load the train and test data\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m read_train(train_data_path)\n\u001b[0;32m     48\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m read_test(test_data_path)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Implement the callback function to track the progress during optimization\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mread_train\u001b[1;34m(train_data_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_train\u001b[39m(train_data_path):\n\u001b[0;32m     10\u001b[0m     mapping \u001b[38;5;241m=\u001b[39m {letter: index \u001b[38;5;28;01mfor\u001b[39;00m index, letter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabcdefghijklmnopqrstuvwxyz\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(train_data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         raw_data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     dataX, dataY \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_train_data.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the paths for the train and test data files\n",
    "train_data_path = \"path_to_your_train_data.txt\"  # Update this path\n",
    "test_data_path = \"path_to_your_test_data.txt\"  # Update this path\n",
    "\n",
    "# Adaptation of read_train function to load training data\n",
    "def read_train(train_data_path):\n",
    "    mapping = {letter: index for index, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "    with open(train_data_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []\n",
    "    \n",
    "    return zip(dataX, dataY)\n",
    "\n",
    "# Adaptation of read_train function to load test data\n",
    "def read_test(test_data_path):\n",
    "    mapping = {letter: index for index, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "    with open(test_data_path, \"r\") as f:\n",
    "        raw_data = f.read().split(\"\\n\")\n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    tempX, tempY = [], []\n",
    "    for row in raw_data[:-1]:\n",
    "        row = row.split(\" \")\n",
    "        tempY.append(mapping[row[1]])\n",
    "        tempX.append(np.array(row[5:], dtype=float))\n",
    "        if int(row[2]) < 0:\n",
    "            dataX.append(np.array(tempX))\n",
    "            dataY.append(np.array(tempY, dtype=int))\n",
    "            tempX, tempY = [], []\n",
    "    \n",
    "    return zip(dataX, dataY)\n",
    "\n",
    "# Now let's load the train and test data\n",
    "train_dataset = read_train(train_data_path)\n",
    "test_dataset = read_test(test_data_path)\n",
    "\n",
    "# Implement the callback function to track the progress during optimization\n",
    "function_evaluations = 0\n",
    "def callback(params, *args):\n",
    "    global function_evaluations\n",
    "    function_evaluations += 1\n",
    "    \n",
    "    # Unpack arguments (assuming test dataset and possibly other args are passed)\n",
    "    X_test, y_test = args[:2]\n",
    "    \n",
    "    # Implement or call your test error function here\n",
    "    test_error = compute_test_error(params, X_test, y_test)\n",
    "    \n",
    "    # Implement or call your training objective function here\n",
    "    training_objective = compute_training_objective(params, X_train, y_train)\n",
    "    \n",
    "    # Print out the current progress\n",
    "    print(f\"Iteration {function_evaluations}: Test Error = {test_error}, Training Objective = {training_objective}\")\n",
    "    \n",
    "    # Optional: Store metrics for later analysis and plotting\n",
    "    test_errors.append(test_error)\n",
    "    train_objectives.append(training_objective)\n",
    "\n",
    "# Initialize lists to store the progress\n",
    "test_errors = []\n",
    "train_objectives = []\n",
    "\n",
    "# Note: You would also need to implement or provide the `compute_test_error` and `compute_training_objective` functions.\n",
    "# Replace \"X_train, y_train\" with the actual variables containing your training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user has asked to solve a problem using stochastic optimization, specifically using SGD and SGD with momentum.\n",
    "# There is a need to implement the algorithms, compare the efficiency of SGD, SGD with momentum, and LBFGS, and plot the results.\n",
    "\n",
    "# Below is the code that sets up the stochastic optimization using SGD and SGD with momentum.\n",
    "# The code assumes that the necessary data loading and preprocessing functions, the objective function, and its gradient are already implemented and available.\n",
    "\n",
    "# Note: This is a high-level pseudo-code and will not run without the full implementation context.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_tnc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume objective_func computes the objective value given the current parameters,\n",
    "# and gradient_func computes its gradient.\n",
    "# SGD and SGD with momentum need to be implemented or imported.\n",
    "# The actual implementations would be more complex and include data loading/preprocessing.\n",
    "\n",
    "# Hyperparameters (to be tuned)\n",
    "learning_rate = 0.01\n",
    "momentum_coeff = 0.9\n",
    "batch_size = 32  # B in the question\n",
    "num_iterations = 1000  # k in the question\n",
    "n = 100  # total number of training examples\n",
    "C = 1000  # fixed value from previous section\n",
    "\n",
    "# Initialization\n",
    "params = np.zeros(parameter_shape)  # Assuming parameter_shape is known\n",
    "velocity = np.zeros(parameter_shape)\n",
    "\n",
    "# Tracking metrics\n",
    "train_objective_values = []\n",
    "test_errors = []\n",
    "\n",
    "# Placeholder for the data loading function\n",
    "# X_train, y_train = load_data(...)\n",
    "# X_test, y_test = load_test_data(...)\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Stochastic mini-batch sampling\n",
    "    indices = np.random.choice(len(X_train), size=batch_size, replace=False)\n",
    "    X_batch, y_batch = X_train[indices], y_train[indices]\n",
    "\n",
    "    # Calculate the gradient using the sampled mini-batch\n",
    "    gradient = gradient_func(params, X_batch, y_batch, C)\n",
    "\n",
    "    # Update rule for SGD with momentum\n",
    "    velocity = momentum_coeff * velocity - learning_rate * gradient\n",
    "    params += velocity\n",
    "\n",
    "    # Update rule for simple SGD (for comparison, one would use one or the other, not both)\n",
    "    params -= learning_rate * gradient\n",
    "\n",
    "    # Calculate and store training objective value\n",
    "    train_objective_value = objective_func(params, X_train, y_train, C)\n",
    "    train_objective_values.append(train_objective_value)\n",
    "\n",
    "    # Calculate and store test error\n",
    "    test_error = compute_test_error(params, X_test, y_test)\n",
    "    test_errors.append(test_error)\n",
    "\n",
    "# Convert lists to arrays for plotting\n",
    "train_objective_values = np.array(train_objective_values)\n",
    "test_errors = np.array(test_errors)\n",
    "\n",
    "# Plotting the decline of training objective value\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_objective_values, label='Training Objective')\n",
    "plt.xlabel('Effective number of passes')\n",
    "plt.ylabel('Training Objective Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the test error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_errors, label='Test Error')\n",
    "plt.xlabel('Effective number of passes')\n",
    "plt.ylabel('Word-wise Test Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Note: The above plots should include curves for LBFGS for comparison, but LBFGS implementation is not included here.\n",
    "# For LBFGS, one would use scipy.optimize.fmin_tnc or a similar function. The implementation would involve additional complexity.\n",
    "\n",
    "# Additional notes:\n",
    "# - The callback function mentioned in the question needs to be implemented to track progress during optimization.\n",
    "# - The 'effective number of passes' is computed as iteration * batch_size / n.\n",
    "# - It's important to tune the learning rate and momentum coefficient to ensure proper convergence.\n",
    "# - The plotting section assumes that effective number of passes and corresponding metrics are correctly calculated and stored.\n",
    "# - This code does not include the logic to handle global variables for callback as mentioned in the question.\n",
    "# - The blog link provided in the question would be used to reference or import implementations of SGD and SGD with momentum, if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming compute_training_objective and compute_test_error are available\n",
    "# Assuming X_train, y_train, X_test, y_test are loaded and available\n",
    "# Assuming parameter_shape is known and appropriate for the CRF model\n",
    "# Assuming gradient_func and objective_func are implemented for the CRF model\n",
    "# The learning_rate, momentum_coeff, batch_size, num_iterations, n, and C should be appropriately set\n",
    "\n",
    "# Initialize parameters for the CRF model\n",
    "params = np.zeros(parameter_shape)\n",
    "velocity = np.zeros(parameter_shape)\n",
    "\n",
    "# Initialize lists to store the progress of training\n",
    "train_objectives_sgd = []\n",
    "train_objectives_momentum = []\n",
    "test_errors_sgd = []\n",
    "test_errors_momentum = []\n",
    "\n",
    "# Function for stochastic gradient descent with and without momentum\n",
    "def run_sgd(X_train, y_train, X_test, y_test, use_momentum=False):\n",
    "    global params, velocity\n",
    "    train_objectives = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Stochastic mini-batch sampling\n",
    "        indices = np.random.choice(len(X_train), size=batch_size, replace=False)\n",
    "        X_batch, y_batch = X_train[indices], y_train[indices]\n",
    "\n",
    "        # Compute the gradient using the sampled mini-batch\n",
    "        gradient = gradient_func(params, X_batch, y_batch, C)\n",
    "        \n",
    "        # Update parameters\n",
    "        if use_momentum:\n",
    "            # Update rule for SGD with momentum\n",
    "            velocity = momentum_coeff * velocity - learning_rate * gradient\n",
    "            params += velocity\n",
    "        else:\n",
    "            # Update rule for simple SGD\n",
    "            params -= learning_rate * gradient\n",
    "        \n",
    "        # Compute training objective value and test error\n",
    "        train_objective = objective_func(params, X_train, y_train, C)\n",
    "        test_error = compute_test_error(params, X_test, y_test)\n",
    "        \n",
    "        # Store the computed values\n",
    "        train_objectives.append(train_objective)\n",
    "        test_errors.append(test_error)\n",
    "    \n",
    "    return train_objectives, test_errors\n",
    "\n",
    "# Run SGD and SGD with momentum\n",
    "train_objectives_sgd, test_errors_sgd = run_sgd(X_train, y_train, X_test, y_test, use_momentum=False)\n",
    "train_objectives_momentum, test_errors_momentum = run_sgd(X_train, y_train, X_test, y_test, use_momentum=True)\n",
    "\n",
    "# Plot training objective decline\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, train_objectives_sgd, label='SGD Training Objective')\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, train_objectives_momentum, label='SGD with Momentum Training Objective')\n",
    "plt.xlabel('Effective number of passes')\n",
    "plt.ylabel('Training Objective Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot test error decline\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, test_errors_sgd, label='SGD Test Error')\n",
    "plt.plot(np.arange(num_iterations) * batch_size / n, test_errors_momentum, label='SGD with Momentum Test Error')\n",
    "plt.xlabel('Effective number of passes')\n",
    "plt.ylabel('Word-wise Test Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# For LBFGS, you would use scipy.optimize.fmin_tnc or similar functions\n",
    "# You would also implement a callback function similar to the one described earlier\n",
    "# The callback would need to track function evaluations, test errors, and training objectives\n",
    "# The plots for LBFGS would need to be added to the above plots for comparison\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
